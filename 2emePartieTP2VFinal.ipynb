{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337efb32",
   "metadata": {},
   "source": [
    "# II- Deuxième partie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb12b48",
   "metadata": {},
   "source": [
    "we modify the prepare_data method to include a text cleaning step: \n",
    "#### Regular Expression:\n",
    "    We use the re.sub function from the re module to replace all non-alphabetic characters in the text with an empty string, effectively removing them.\n",
    "\n",
    "#### Cleaning Step:\n",
    "    The cleaning step is performed on the entire text before splitting it into sentences. This ensures that non-alphabetic characters are removed from the entire corpus.\n",
    "\n",
    "#### Updated Data Preparation:\n",
    "    After cleaning, the rest of the prepare_data method remains the same, processing the cleaned text to generate the preprocessed corpus for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1172dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31fc0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.trigram_counts = defaultdict(int)\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.k = 0.01  # Smoothing parameter\n",
    "        self.start_token = '<s>'\n",
    "        self.end_token = '</s>'\n",
    "\n",
    "    def prepare_data(self, data, ngram_size=2, is_file=True):\n",
    "        if is_file:\n",
    "            with open(data, 'r') as f:\n",
    "                text = f.read().lower()\n",
    "        else:\n",
    "            text = data.lower()\n",
    " \n",
    "        # Clean the text to remove non-alphabetic characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        sentences = text.split('\\n')\n",
    "        preprocessed_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            if ngram_size == 2:\n",
    "                tokens = [self.start_token] + tokens + [self.end_token]\n",
    "            elif ngram_size == 3:\n",
    "                tokens = [self.start_token, self.start_token] + tokens + [self.end_token]\n",
    "            preprocessed_sentences.append(' '.join(tokens))\n",
    "\n",
    "        preprocessed_corpus = ' '.join(preprocessed_sentences)\n",
    "\n",
    "        words = preprocessed_corpus.split()\n",
    "        word_counts = Counter(words)\n",
    "        self.vocab = {word for word in word_counts if word_counts[word] >= 1}\n",
    "        self.vocab.add('<UNK>')\n",
    "\n",
    "        def replace_oov(word):\n",
    "            return word if word in self.vocab else '<UNK>'\n",
    "\n",
    "        preprocessed_corpus = ' '.join(replace_oov(word) for word in words)\n",
    "\n",
    "        return preprocessed_corpus\n",
    "\n",
    "    def train(self, ngram_size=2, infile='shakespeare.txt'):\n",
    "        preprocessed_corpus = self.prepare_data(infile, ngram_size, is_file=True)\n",
    "        tokens = preprocessed_corpus.split()\n",
    "\n",
    "        if ngram_size == 2:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                self.bigram_counts[(tokens[i], tokens[i+1])] += 1\n",
    "        elif ngram_size == 3:\n",
    "            for i in range(len(tokens) - 2):\n",
    "                self.trigram_counts[(tokens[i], tokens[i+1], tokens[i+2])] += 1\n",
    "\n",
    "    def predict_ngram(self, sentence, ngram_size=2):\n",
    "        preprocessed_sentence = self.prepare_data(sentence, ngram_size, is_file=False)\n",
    "        tokens = preprocessed_sentence.split()\n",
    "        log_prob = 0.0\n",
    "\n",
    "        if ngram_size == 2:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                log_prob += self.calculate_log_prob_bigram(tokens[i], tokens[i+1])\n",
    "        elif ngram_size == 3:\n",
    "            for i in range(len(tokens) - 2):\n",
    "                log_prob += self.calculate_log_prob_trigram(tokens[i], tokens[i+1], tokens[i+2])\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def calculate_log_prob_bigram(self, word1, word2):\n",
    "        count_bigram = self.bigram_counts[(word1, word2)]\n",
    "        count_unigram = sum(self.bigram_counts[(word1, w)] for w in self.vocab)\n",
    "        vocab_size = len(self.vocab)\n",
    "        prob = (count_bigram + self.k) / (count_unigram + self.k * vocab_size)\n",
    "        return math.log(prob)\n",
    "\n",
    "    def calculate_log_prob_trigram(self, word1, word2, word3):\n",
    "        count_trigram = self.trigram_counts[(word1, word2, word3)]\n",
    "        count_bigram = sum(self.trigram_counts[(word1, word2, w)] for w in self.vocab)\n",
    "        vocab_size = len(self.vocab)\n",
    "        prob = (count_trigram + self.k) / (count_bigram + self.k * vocab_size)\n",
    "        return math.log(prob)\n",
    "\n",
    "    def test_perplexity(self, test_file, ngram_size=2):\n",
    "        total_log_prob = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with open(test_file, 'r') as f:\n",
    "            for line in f:\n",
    "                sentence = line.strip().lower()\n",
    "                total_log_prob += self.predict_ngram(sentence, ngram_size)\n",
    "                total_tokens += len(sentence.split()) + 1  # Adding 1 for the end token\n",
    "\n",
    "        avg_log_prob = total_log_prob / total_tokens\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        return perplexity\n",
    "\n",
    "    def generate_text(self, ngram_size=2, max_length=200):\n",
    "        if ngram_size == 2:\n",
    "            current_token = self.start_token\n",
    "            text = []\n",
    "            sentence_count = 0\n",
    "            for _ in range(max_length):\n",
    "                next_token = self.sample_next_word(current_token, ngram_size)\n",
    "                if next_token == self.end_token:\n",
    "                    sentence_count += 1\n",
    "                    if sentence_count >= 8:\n",
    "                        break\n",
    "                    current_token = self.start_token\n",
    "                else:\n",
    "                    text.append(next_token)\n",
    "                    current_token = next_token\n",
    "            return ' '.join(text)\n",
    "        elif ngram_size == 3:\n",
    "            current_tokens = [self.start_token, self.start_token]\n",
    "            text = []\n",
    "            sentence_count = 0\n",
    "            for _ in range(max_length):\n",
    "                next_token = self.sample_next_word(tuple(current_tokens), ngram_size)\n",
    "                if next_token == self.end_token:\n",
    "                    sentence_count += 1\n",
    "                    if sentence_count >= 8:\n",
    "                        break\n",
    "                    current_tokens = [self.start_token, self.start_token]\n",
    "                else:\n",
    "                    text.append(next_token)\n",
    "                    current_tokens = [current_tokens[1], next_token]\n",
    "            return ' '.join(text)\n",
    "\n",
    "    def sample_next_word(self, current_token, ngram_size):\n",
    "        if ngram_size == 2:\n",
    "            candidates = [(key[1], self.bigram_counts[key]) for key in self.bigram_counts if key[0] == current_token]\n",
    "        elif ngram_size == 3:\n",
    "            candidates = [(key[2], self.trigram_counts[key]) for key in self.trigram_counts if (key[0], key[1]) == current_token]\n",
    "\n",
    "        if not candidates:\n",
    "            return self.end_token\n",
    "\n",
    "        words, counts = zip(*candidates)\n",
    "        total = sum(counts)\n",
    "        probs = [count / total for count in counts]\n",
    "\n",
    "        return np.random.choice(words, p=probs)\n",
    "\n",
    "    def auto_complete(self, prefix, ngram_size=2):\n",
    "        tokens = prefix.lower().split()\n",
    "        if ngram_size == 2:\n",
    "            last_token = tokens[-1]\n",
    "            next_word = self.sample_next_word(last_token, ngram_size)\n",
    "        elif ngram_size == 3:\n",
    "            last_two_tokens = tokens[-2:]\n",
    "            next_word = self.sample_next_word(tuple(last_two_tokens), ngram_size)\n",
    "        return next_word\n",
    "\n",
    "    def correction(self, word, ngram_size=2):\n",
    "        # This method requires additional implementation for spelling correction\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40ff07",
   "metadata": {},
   "source": [
    "#### Initialisation du modèle:\n",
    "\n",
    "crée une instance de NgramLanguageModel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ba6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation :\n",
    "model = NgramLanguageModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce503b",
   "metadata": {},
   "source": [
    "### Entraînement du modèle:bigramme\n",
    "\n",
    " entraîne le modèle bigramme avec le fichier shakespeare.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5133e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(ngram_size=2, infile='big_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8f75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Bigram) on test file: 8.273168431638918\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (Bigram) on test file:\", model.test_perplexity(test_file='big_data.txt', ngram_size=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7277b3",
   "metadata": {},
   "source": [
    "#### Génération de texte:\n",
    "\n",
    "génère un texte basé sur le modèle bigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577ba8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Bigram): love your favorite kurt vile and technical and bricks at second halfksu mu what they not and tuesdays does not the sidewalk smash isnt known defeat of happiness never shout out the stock bump on the super early tmrwww youknowlifeisgreatwhen years since they could i like the rt i will pass out the tweets love youu but we will also looks like chandler man what call tonightthanks and jerome bronson doing down in atlanta will outperform nat library want a thug by great time\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text (Bigram):\", model.generate_text(ngram_size=2, max_length=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9988ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Bigram): i have a nugget nectar by earnings or sun hahahah dude im moving forward to classy very interestinq wow mobile web are moving truck will be for ne hes all craft beer seems as great meeting after one hell and nuts it digging the concert thanks for c gallagher will be great job min left evr was a really fun and then not the tivo in the tweets i miss free ebook exhibitors out to s do but youre obviously very well that there ever think helping peers out on the grammar as seconds of dust\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text (Bigram):\", model.generate_text(ngram_size=2, max_length=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b08cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Bigram): im stuck in the softball game eh walking youre driving more than vic for them saw that needs to control when its so i should be pushed us about it a good one of a super day yea did reason these prom cool and prayers my cincinnati its the worst well he was great to make it was vanilla pudding milkshake steve perry and the crosby sweepstakes today i wish the cursive it makes no possibility of project in mn for nola off the ass\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text (Bigram):\", model.generate_text(ngram_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e97aa",
   "metadata": {},
   "source": [
    "#### Auto-complétion:\n",
    " prédit le mot suivant le plus probable après \"I am\" en utilisant le modèle bigramme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2587ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'I am': dizzy\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'I am':\", model.auto_complete('I am', ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cde2448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'Please': call\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'Please':\", model.auto_complete('Please', ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae77dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'I ': havent\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'I ':\", model.auto_complete('I', ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84a806ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'we were': amazing\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'we were':\", model.auto_complete('we were', ngram_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2beed673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'do ': any\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'do ':\", model.auto_complete('do', ngram_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f872923",
   "metadata": {},
   "source": [
    "### Entraînement du modèle:¶ Trigramme \n",
    "entraîne le modèle Trigramme avec le fichier shakespeare.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ddf651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(ngram_size=3, infile='big_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9eb97973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Trigram) on test file: 2.0651289642857544\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (Trigram) on test file:\", model.test_perplexity(test_file='big_data.txt', ngram_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60967613",
   "metadata": {},
   "source": [
    "#### Génération de texte:\n",
    "\n",
    " génère un texte basé sur le modèle Trigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "280c880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Trigram): i do is win congrats jim good luck finding this one a tiny attic or crawl space if i slam this manila folder into the playoffs so the beast can show you what the fuck brings grenades to a soccer goal obviously i lost it in israel and he alone is responsible for what if you want to marry him some love and i will be a reviewer let us know if you make my way there are certain colors that dont walk like see your rendition of swipes on swipes lol im glad we have is better than the seeing of dolls murder at the love of all you want to try peeps next time you have to be around much today kiss your mommy for me great picture the green shirt with ur shortcmings they will learn anyway can u chew gum walk a mile in his songs he takes lots of fun see you tonight\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text (Trigram):\", model.generate_text(ngram_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a6dede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Trigram): seal team rocks congrats on a shoot out salute follow him for a reason not to mention a masterpiece of reptilian westerns i am on twitter in was crazy sheesh just think about that one thing iss on another week then the next one lakers stop making excuses for kobe since they are eating may be the one night and invite people over hmm gotta pick a subject area or just a few new songs i wanna go tanning again uh gmorning good to their purpose thats what its all mine muahahha yes i did with it\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text (Trigram):\", model.generate_text(ngram_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a98ec",
   "metadata": {},
   "source": [
    "#### Auto-complétion:\n",
    "\n",
    "prédit le mot suivant le plus probable en utilisant le modèle Trigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6635707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'I am': crazy\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'I am':\", model.auto_complete('I am', ngram_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3389536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'please': still\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'do you':\", model.auto_complete('do you', ngram_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8b9fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete for 'do you': ever\n"
     ]
    }
   ],
   "source": [
    "print(\"Autocomplete for 'do you':\", model.auto_complete('do you', ngram_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c245ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
